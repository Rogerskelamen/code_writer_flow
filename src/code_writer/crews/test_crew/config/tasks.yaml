test_write_task:
  description: >
    Review the Python function below and according to {requirement},
    write unit tests for it using assert statements.
    Ensure that the tests cover all edge cases and validate the function's behavior.
    {code}
  expected_output: >
    A Python code that includes a set of unit tests for the Python function
    that validate its correctness and the function itself.
    Each test should be clear and concise, using assert statements to check the function's output.
    The whole code should be well-structured and could be run directly.
    Formatted as python code without '```'
  agent: test_developer
  output_file: output/tests.py

test_execution_task:
  description: >
    Run the test file 'output/tests.py' that was generated by the test developer.
    Use the Test Runner tool to execute the tests and provide detailed feedback about:
    1. Whether all tests passed or failed
    2. Any error messages or issues found
    3. Suggestions for fixing any failing tests
    4. Overall assessment of code quality based on test results
  expected_output: >
    A comprehensive test execution report that includes:
    - Test execution results (passed/failed tests)
    - Any error messages or stack traces
    - Analysis of test coverage and code quality
    - Recommendations for improvements if tests fail
    - Summary of overall code reliability
  agent: test_runner
  context:
    - test_write_task

code_fix_task:
  description: >
    Based on the test execution results, analyze any failures and fix the original code.
    You have access to:
    1. The original code from the program_task
    2. The test results and failure analysis from test_execution_task
    3. The test code for understanding expected behavior

    If any test failed:
    - Identify the root cause of failures
    - Fix the original code to make all tests pass
    - Ensure the fix is minimal and maintains the original intent
    - Explain what was wrong and how you fixed it

    If all tests passed:
    - Confirm the code is working correctly
    - Suggest any potential improvements for robustness
    - Return the original code with any minor optimizations
  expected_output: >
    If tests failed:
    Output fixed Python code that addresses all test failures.

    If all tests passed:
    Output the original Python code.

    The output should be a json data formatted as:
    ```json
    {
      "code": "clean Python code(a function)", # The fixed or original code
      "passed": "true"  # Whether all tests passed
    }
    ```
  agent: code_fixer
  context:
    - test_execution_task
